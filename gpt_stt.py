
import time
import numpy as np
import stt_listen
import pyaudio
import wave
import tempfile
import cv2
import threading
from PIL import ImageFont, ImageDraw, Image
import torch


CHUNK = 2048               # ë²„í¼ í¬ê¸° 2048ë³´ë‹¤ ì‘ìœ¼ë©´ ëŠê¹€ ë°œìƒ
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000               # Whisper ê¶Œì¥ ìƒ˜í”Œë ˆì´íŠ¸
DURATION = 7               # ê¸°ë³¸ ë…¹ìŒ ì‹œê°„(ì´ˆ)

# Whisper ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ : ë…¹ìŒê³¼ ëª¨ë¸ ë¡œë“œê°€ ë™ì‹œì— ì¼ì–´ë‚˜ë©´ ëª¨ë¸ ë¡œë”©(gpuìì› ë…ì ) ë•Œë¬¸ì— ë…¹ìŒ ëŠê¹€ì´ ë°œìƒí•¨ + ë¯¸ë¦¬ ë¡œë“œí•˜ë©´ ë…¹ìŒ í›„ ëª¨ë¸ ì—°ì‚° ì†ë„ê°€ ë¹¨ë¼ì§ 
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print("ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤:", device)
# model = whisper.load_model("medium")  # 4080ì—ì„œëŠ” medium ëª¨ë¸ ê¶Œì¥ì¸ë°, large-v3ê°€ ë” ì„±ëŠ¥ì´ ì¢‹ìŒ
# print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")


def record_audio(duration=DURATION, sample_rate=RATE):

    # ë…¹ìŒ ì¤‘ ì˜ìƒ ì „ì²´í™”ë©´ ì¬ìƒ (ffplay)
    import subprocess
    video_path = r"C:\Artech5\Image_Box\mic_listening.mp4"
    ffplay_bin = "ffplay"
    cmd = [ffplay_bin, "-autoexit", "-fs", "-loglevel", "error", video_path]
    video_proc = subprocess.Popen(cmd)
    # # ì¹´ë©”ë¼ ë¨¼ì € ì¼œê¸°
    # stop_event = threading.Event()
    # cam_thread = threading.Thread(target=camera_thread_func, args=(stop_event,), daemon=True)
    # cam_thread.start()
    # time.sleep(0.5)  # ì¹´ë©”ë¼ ì•ˆì •í™”


    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS, rate=sample_rate,
                    input=True, frames_per_buffer=CHUNK)

    print("ğŸ™ï¸ ë…¹ìŒ ì‹œì‘...")
    frames = []

    for _ in range(0, int(sample_rate / CHUNK * duration)):
        data = stream.read(CHUNK, exception_on_overflow=False)  # ëŠê¹€ ì™„í™”
        frames.append(data)

    print("âœ… ë…¹ìŒ ì™„ë£Œ.")

    stream.stop_stream()
    stream.close()
    p.terminate()

    # ë…¹ìŒ ëë‚˜ê³  ì˜ìƒ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
    video_proc.wait()

    # ì„ì‹œ wav ì €ì¥
    temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    with wave.open(temp_wav.name, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(sample_rate)
        wf.writeframes(b''.join(frames))

    return temp_wav.name



def STT_whisper(audio_path, whisper_model):
    print("ğŸ“ Whisper STT ì‹¤í–‰ ì¤‘...")
    result = whisper_model.transcribe(audio_path, language="ko")
    return result["text"]


# ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  gpt_listen í•¨ìˆ˜ : ì•ì„œ ë¡œë“œëœ whisper ëª¨ë¸ì„ ì‚¬ìš©
def gpt_listen(duration: int, whisper_model) -> str:

    audio_file_path = record_audio(duration=duration)
    print(f"ğŸ“‚ ë…¹ìŒ íŒŒì¼ ê²½ë¡œ: {audio_file_path}")
    transcribed_text = STT_whisper(audio_file_path, whisper_model)
    print(f"ğŸ’¬ ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcribed_text}")
    return transcribed_text


# ì‹¤í–‰ë¶€
if __name__ == "__main__":
    audio_file_path = record_audio(duration=7)  # ë…¹ìŒ ì‹¤í–‰
    print(f"ğŸ“‚ ë…¹ìŒ íŒŒì¼ ê²½ë¡œ: {audio_file_path}")

    transcribed_text = STT_whisper(audio_file_path)
    print(f"ğŸ’¬ ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcribed_text}")
